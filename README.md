<div align="center">
<h1>Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes</h1>

[Nikita Kiselev](https://github.com/kisnikser)<sup>1, 2 :email:</sup>, [Andrey Grabovoy](https://github.com/andriygav)<sup>1</sup>

<sup>1</sup> Moscow Institute of Physics and Technology, <sup>2</sup> Sber AI

<sup>:email:</sup> Corresponding author

[ğŸ“ Paper](https://github.com/kisnikser/landscape-hessian/blob/main/paper/main.pdf), [</> Code](https://github.com/kisnikser/landscape-hessian/tree/main/code), [ğŸª§ Slides](https://github.com/kisnikser/landscape-hessian/blob/main/slides/main.pdf)

</div>

## ğŸ’¡ Abstract
The loss landscape of neural networks is a critical aspect of their behavior, and understanding its properties is essential for improving their performance. 
In this paper, we investigate how the loss surface changes when the sample size increases, a previously unexplored issue. 
We theoretically analyze the convergence of the loss landscape in a fully connected neural network and derive upper bounds for the difference in loss function values when adding a new object to the sample. 
Our empirical study confirms these results on various datasets, demonstrating the convergence of the loss function surface for image classification tasks. 
Our findings provide insights into the local geometry of neural loss landscapes and have implications for the development of sample size determination techniques.

## ğŸ” Overview
<div align="center">
  <img alt="overview" src="paper/losses_difference.png">
</div>


## ğŸ› ï¸ Repository Structure
The repository is structured as follows:
- `paper`: This directory contains the main paper in PDF format (`main.pdf`) and the LaTeX source file (`main.tex`). Also there are directories `figs` and `figs_extraction` with images used in the paper.
- `code`: This directory contains the code used in the paper. It has its own `README.md` file providing a detailed description of the code files.
```shell
landscape-hessian
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ code
â”‚Â Â  â”œâ”€â”€ configs
â”‚Â Â  â”œâ”€â”€ figs
â”‚Â Â  â”œâ”€â”€ figs_extaction
â”‚Â Â  â”œâ”€â”€ results
â”‚Â Â  â”œâ”€â”€ results_extraction
â”‚Â Â  â”œâ”€â”€ get_loss_values_extraction.py
â”‚Â Â  â”œâ”€â”€ get_loss_values.py
â”‚Â Â  â”œâ”€â”€ plot_differences_extraction.py
â”‚Â Â  â””â”€â”€ plot_differences.py
â””â”€â”€  paper
 Â Â  â”œâ”€â”€ figs
 Â Â  â”œâ”€â”€ figs_extraction
 Â Â  â”œâ”€â”€ losses_difference.pdf
 Â Â  â”œâ”€â”€ losses_difference.png
    â”œâ”€â”€ main.pdf
 Â Â  â”œâ”€â”€ main.tex
 Â Â  â”œâ”€â”€ neurips_2024.sty
 Â Â  â”œâ”€â”€ new_commands.tex
 Â Â  â””â”€â”€ references.bib
```

## ğŸ“š Citation
```BibTeX
@article{kiselev2024unraveling,
  title={Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes},
  author={Kiselev, Nikita and Grabovoy, Andrey},
  journal={arXiv preprint arXiv:2409.11995},
  year={2024}
}
```
